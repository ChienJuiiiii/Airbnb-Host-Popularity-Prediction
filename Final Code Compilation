{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"ir","display_name":"R"},"language_info":{"name":"R"}},"cells":[{"cell_type":"markdown","source":["# Group 11 - Code Compilation\n","## Chien-Jui Huang, Jie Yun Guan, Pravah Malunjkar, Navya Gehlot, Gautam Bhatia"],"metadata":{"id":"XHB-qlZ3dCH5"}},{"cell_type":"markdown","source":["## Load Dataset"],"metadata":{"id":"3SXQUwzohY1_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MZc8JNFc5Te"},"outputs":[],"source":["#Load Libraries\n","library(tidyverse)\n","library(caret)\n","library(class)\n","library(dplyr)\n","library(glmnet)\n","library(pROC)\n","library(text2vec)\n","library(tm)\n","library(SnowballC)\n","library(randomForest)\n","library(gbm)\n","library(ISLR)\n","library(corrplot)\n","library(ggplot2)\n","library(reshape2)\n","library(stringr)\n","library(vip)"]},{"cell_type":"code","source":["#Load Data Files\n","setwd(\"C:/Users/malun/Documents/UMD Grad Spring '24/BUDT 758T Data Mining and Predictive Analytics/BUDT 758T Final Project\")\n","train_x <- read_csv(\"airbnb_train_x_2024.csv\")\n","train_y <- read_csv(\"airbnb_train_y_2024.csv\")\n","test_x <- read_csv(\"airbnb_test_x_2024.csv\")"],"metadata":{"id":"wK9clVBNdzMH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Correlation Matrix"],"metadata":{"id":"PkuUag_Yd6_L"}},{"cell_type":"code","source":["################################# Check Correlation #################################\n","numeric_train_x_cleaned <- select_if(train_x, is.numeric)\n","correlation_matrix <- cor(numeric_train_x_cleaned, use = \"complete.obs\")\n","print(correlation_matrix)\n","corrplot(correlation_matrix, method = \"circle\", type = \"upper\", order = \"hclust\",\n","         tl.cex = 0.6, tl.col = \"black\")\n","\n","# Set the threshold for correlation value\n","correlation_threshold <- 0.3\n","\n","# Calculate absolute values of correlation matrix\n","abs_correlation_matrix <- abs(correlation_matrix)\n","\n","# Initialize an empty list to store correlated feature pairs\n","correlated_feature_pairs <- list()\n","\n","# Iterate through the correlation matrix to find feature pairs\n","for (i in 1:(ncol(abs_correlation_matrix) - 1)) {\n","  for (j in (i + 1):ncol(abs_correlation_matrix)) {\n","    if (abs_correlation_matrix[i, j] > correlation_threshold) {\n","      # Store the names of correlated features\n","      feature1 <- colnames(abs_correlation_matrix)[i]\n","      feature2 <- colnames(abs_correlation_matrix)[j]\n","      correlated_feature_pairs[[paste(feature1, feature2, sep = \" & \")]] <- abs_correlation_matrix[i, j]\n","    }\n","  }\n","}\n","\n","# Print correlated feature pairs with their absolute correlation values\n","print(\"Pairs of features with absolute correlation > 0.3:\")\n","for (pair in names(correlated_feature_pairs)) {\n","  cat(pair, \": \", correlated_feature_pairs[[pair]], \"\\n\")\n","}"],"metadata":{"id":"rBhp6j6zd4Nn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Cleaning, Manipulation and Processing"],"metadata":{"id":"qhKuc7oHecgx"}},{"cell_type":"code","source":["# Add a column to each data frame to differentiate train and test data\n","train_x$dataset <- \"train\"\n","test_x$dataset <- \"test\"\n","combined_data <- bind_rows(train_x, test_x)"],"metadata":{"id":"HKEmp2BZeakp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################# Clean Train/Test Data #################################\n","combined_data_cleaned <- combined_data %>%\n","  # Clean categorical features\n","  mutate(\n","    bed_type = factor(bed_type),\n","    cancellation_policy = factor(ifelse(cancellation_policy %in% c(\n","      \"strict\", \"super_strict_30\", \"super_strict_60\", \"no_refunds\"), \"strict\", cancellation_policy)),\n","    license = factor(ifelse(is.na(license), 0, 1))\n","  ) %>%\n","  mutate(market = factor(ifelse(is.na(market), \"OTHER\", market))) %>%\n","  mutate(\n","    property_type = factor(ifelse(is.na(property_type), \"Apartment\", property_type)),\n","    property_type = factor(case_when(\n","      property_type %in% c(\"Apartment\", \"Serviced apartment\", \"Loft\") ~ \"apartment\",\n","      property_type %in% c(\"Bed & Breakfast\", \"Boutique hotel\", \"Hostel\") ~ \"hotel\",\n","      property_type %in% c(\"Townhouse\", \"Condominium\") ~ \"condo\",\n","      property_type %in% c(\"Bungalow\", \"House\") ~ \"house\",\n","      TRUE ~ \"other\"\n","    )),\n","    room_type = factor(room_type),\n","    state = factor(toupper(state))\n","  ) %>%\n","  select(-city, -country, -country_code, -experiences_offered, -host_location,\n","         -host_neighbourhood, -host_response_time, -jurisdiction_names, -neighborhood,\n","         -neighborhood_group, -smart_location,\n","         -first_review) %>%\n","  # Clean numeric data\n","  mutate(\n","    accommodates = ifelse(is.na(accommodates), median(accommodates, na.rm = TRUE), accommodates),\n","    availability_365 = factor(ifelse(availability_365 > 0, 1, 0)),\n","    bathrooms = ifelse(is.na(bathrooms), median(bathrooms, na.rm = TRUE), bathrooms),\n","    bedrooms = ifelse(is.na(bedrooms), median(bedrooms, na.rm = TRUE), bedrooms),\n","    beds = ifelse(is.na(beds), median(beds, na.rm = TRUE), beds),\n","    cleaning_fee = ifelse(is.na(cleaning_fee), 0, cleaning_fee),\n","    cleaning_fee = factor(ifelse(cleaning_fee > 0, 1,0)),\n","    extra_people = round(log(ifelse(is.na(extra_people), 0, extra_people)) ,0),\n","    extra_people = ifelse(extra_people <=0, 1, extra_people),\n","    host_acceptance = factor(case_when(\n","      host_acceptance_rate == 100 ~ \"ALL\",\n","      host_acceptance_rate < 100 ~ \"HIGH\",\n","      host_acceptance_rate < 50 ~ \"MODERATE\",\n","      TRUE ~ \"MISSING\")),\n","    # host_listings_count non-monotonic relationship with target label\n","    log_host_listings_count = round(log(ifelse(is.na(host_listings_count), 0, host_listings_count)), 0),\n","    HLC_bin = cut(log_host_listings_count, breaks = 6,\n","                  labels = c(\"HLC1\", \"HLC2\", \"HLC3\", \"HLC4\", \"HLC5\", \"HLC6\")),\n","    host_response = factor(case_when(\n","      host_response_rate == 100 ~ \"ALL\",\n","      host_response_rate < 100 ~ \"SOME\",\n","      TRUE ~ \"MISSING\")),\n","    log_host_total_listings_count =  round(log(ifelse(is.na(host_total_listings_count), 0, host_total_listings_count)), 0),\n","    log_host_total_listings_count = ifelse(log_host_total_listings_count <= 0, 1, log_host_total_listings_count),\n","    log_maximum_nights = round(log(ifelse(is.na(maximum_nights), 0 ,maximum_nights)),0),\n","    log_maximum_nights = case_when(\n","      log_maximum_nights == 0 ~ 1,\n","      log_maximum_nights >= 7 ~ 6,\n","      TRUE ~ log_maximum_nights),\n","    log_minimum_nights = round(log(ifelse(is.na(minimum_nights), 0 ,minimum_nights)),0),\n","    log_minimum_nights = factor(case_when(\n","      log_minimum_nights >= 3 ~ 3,\n","      log_minimum_nights >= 1 ~ log_minimum_nights,\n","      log_minimum_nights <= 1 ~ 0,\n","      TRUE ~ log_minimum_nights)),\n","    monthly_price = ifelse(is.na(monthly_price), 0, monthly_price),\n","    monthly_price = ifelse(monthly_price < price*30 & monthly_price != 0, 1, 0),\n","    monthly_price = factor(ifelse(is.na(monthly_price), 0, monthly_price)),\n","    log_price = round(log(ifelse(is.na(price), median(price, na.rm = TRUE), price)),0),\n","    log_price = ifelse(log_price <= 0 , 1, log_price),\n","    security_deposit = ifelse(is.na(security_deposit), 0, security_deposit),\n","    security_deposit =  factor(ifelse(security_deposit > 0, 1, 0))\n","  ) %>%\n","  select(-availability_30, -availability_60, -availability_90,\n","         -square_feet, -weekly_price, -zipcode,\n","         -host_response_rate, -host_acceptance_rate, -host_listings_count,\n","         -host_total_listings_count, -maximum_nights, -minimum_nights, -price,\n","         -guests_included, -log_host_listings_count) %>%\n","  # wendy feature engineering\n","  mutate(\n","    host_since_days = as.integer(difftime(Sys.Date(), host_since, units = \"days\")),\n","    host_since_days = ifelse(is.na(host_since_days), median(host_since_days, na.rm = TRUE), host_since_days),\n","    description_length = ifelse(is.na(description), 0, nchar(as.character(description))),\n","    pets_allowed = factor(ifelse(grepl(\"Pets allowed\", amenities), 1, 0))\n","  ) %>%\n","  # jerry feature engineering\n","  mutate(\n","    neighborhood_overview = ifelse(is.na(neighborhood_overview), \"none\", neighborhood_overview),\n","    neighborhood_overview = tolower(neighborhood_overview),\n","    is_market_enterain = factor(\n","      ifelse(grepl(\"whole foods|walmart|giants|lidl|aldi|groceries|grocery|\n","                   supermarket|bar|costco|target|music|restaurants|cafes|\n","                   coffee|food|shopping\",neighborhood_overview),1, 0)),\n","    is_interaction = factor(ifelse(is.na(interaction), 0, 1)),\n","    house_rules = ifelse(is.na(interaction), \"none\", house_rules),\n","    house_rules = tolower(house_rules),\n","    house_rules_no_count = round(log(str_count(house_rules, \"no\")),0),\n","    house_rules_no_count = factor(case_when(\n","      house_rules_no_count >= 1 ~ 1,\n","      house_rules_no_count < 1 ~ 0,\n","      TRUE ~ 0\n","    ))) %>%\n","  # pravah feature engineering\n","  mutate(\n","    summary_length = ifelse(is.na(summary), 0, nchar(as.character(summary))),\n","    host_about = factor(ifelse(is.na(host_about), 0, 1)),\n","    house_rules = factor(ifelse(is.na(house_rules), 0, 1))\n","  ) %>%\n","  # gautam feature engineering\n","  mutate(amenities_count = sapply(strsplit(as.character(amenities), \",\"), length),\n","         host_verifications_count = sapply(strsplit(as.character(host_verifications), \",\"), length)\n","  ) %>%\n","  # clean text feature\n","  select(-access, -description, -features, -host_about, -host_name, -house_rules,\n","         -interaction, -name, -neighborhood_overview, -notes, -space, -street,\n","         -summary, -transit, -host_since, -amenities, -host_verifications)\n","\n","# Define a function to determine timezone based on latitude and longitude\n","determine_timezone <- function(lat, long) {\n","  if (lat >= 24 & lat <= 47 & long >= -85 & long <= -67) {\n","    return(\"ET\")\n","  } else if (lat >= 25 & lat <= 49 & long >= -103 & long <= -85) {\n","    return(\"CT\")\n","  } else if (lat >= 29 & lat <= 49 & long >= -115 & long <= -102) {\n","    return(\"MT\")\n","  } else if (lat >= 32 & lat <= 49 & long >= -125 & long <= -114) {\n","    return(\"PT\")\n","  } else if (lat >= 54 & long >= -172 & long <= -130) {\n","    return(\"AKT\")\n","  } else if (lat >= 19 & lat <= 28 & long >= -178 & long <= -154) {\n","    return(\"HAT\")\n","  } else {\n","    return(\"OTHER\")\n","  }\n","}\n","\n","# Apply the function to create the timezone column\n","combined_data_cleaned$timezone <- mapply(determine_timezone, combined_data_cleaned$latitude, combined_data_cleaned$longitude)\n","combined_data_cleaned <- combined_data_cleaned %>%\n","  mutate(timezone = factor(timezone)) %>%\n","  select(-latitude, -longitude)\n","\n","summary(combined_data_cleaned)\n","colnames(combined_data_cleaned)"],"metadata":{"id":"-siZps0Mem2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################ Normalize Combined Data ################################\n","min_accommodates <- min(combined_data_cleaned$accommodates)\n","max_accommodates <- max(combined_data_cleaned$accommodates)\n","combined_data_cleaned$accommodates <- (combined_data_cleaned$accommodates - min_accommodates) / (max_accommodates - min_accommodates)\n","\n","min_bathrooms <- min(combined_data_cleaned$bathrooms)\n","max_bathrooms <- max(combined_data_cleaned$bathrooms)\n","combined_data_cleaned$bathrooms <- (combined_data_cleaned$bathrooms - min_bathrooms) / (max_bathrooms - min_bathrooms)\n","\n","min_bedrooms <- min(combined_data_cleaned$bedrooms)\n","max_bedrooms <- max(combined_data_cleaned$bedrooms)\n","combined_data_cleaned$bedrooms <- (combined_data_cleaned$bathrooms - min_bedrooms) / (max_bedrooms - min_bedrooms)\n","\n","min_beds <- min(combined_data_cleaned$beds)\n","max_beds <- max(combined_data_cleaned$beds)\n","combined_data_cleaned$beds <- (combined_data_cleaned$beds - min_beds) / (max_beds - min_beds)\n","\n","min_extra_people <- min(combined_data_cleaned$extra_people)\n","max_extra_people <- max(combined_data_cleaned$extra_people)\n","combined_data_cleaned$extra_people <- (combined_data_cleaned$extra_people - min_extra_people) / (max_extra_people - min_extra_people)\n","\n","min_log_host_total_listings_count <- min(combined_data_cleaned$log_host_total_listings_count)\n","max_log_host_total_listings_count <- max(combined_data_cleaned$log_host_total_listings_count)\n","combined_data_cleaned$log_host_total_listings_count <- (combined_data_cleaned$log_host_total_listings_count - min_log_host_total_listings_count) / (max_log_host_total_listings_count - min_log_host_total_listings_count)\n","\n","min_log_maximum_nights <- min(combined_data_cleaned$log_maximum_nights)\n","max_log_maximum_nights <- max(combined_data_cleaned$log_maximum_nights)\n","combined_data_cleaned$log_maximum_nights <- (combined_data_cleaned$log_maximum_nights - min_log_maximum_nights) / (max_log_maximum_nights - min_log_maximum_nights)\n","\n","min_host_since_days <- min(combined_data_cleaned$host_since_days)\n","max_host_since_days <- max(combined_data_cleaned$host_since_days)\n","combined_data_cleaned$host_since_days <- (combined_data_cleaned$host_since_days - min_host_since_days) / (max_host_since_days - min_host_since_days)\n","\n","min_description_length <- min(combined_data_cleaned$description_length)\n","max_description_length <- max(combined_data_cleaned$description_length)\n","combined_data_cleaned$description_length <- (combined_data_cleaned$description_length - min_description_length) / (max_description_length - min_description_length)\n","\n","min_summary_length <- min(combined_data_cleaned$summary_length)\n","max_summary_length <- max(combined_data_cleaned$summary_length)\n","combined_data_cleaned$summary_length <- (combined_data_cleaned$summary_length - min_summary_length) / (max_summary_length - min_summary_length)\n","\n","summary(combined_data_cleaned)"],"metadata":{"id":"0hjtstFtevEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################# Clean Text Feature #################################\n","combined_data$id <- 1:nrow(combined_data)\n","\n","cleaning_tokenizer <- function(v) {\n","  v %>%\n","    removeNumbers() %>%\n","    removePunctuation() %>%\n","    removeWords(tm::stopwords(kind = \"en\")) %>%\n","    word_tokenizer()\n","}\n","\n","process_text_feature <- function(data_frame, feature_name, vocab_term_max = 10, term_count_min = NULL, doc_proportion_max = NULL) {\n","  # Replace separators with space\n","  data_frame[[feature_name]] <- gsub(\",\", \" \", data_frame[[feature_name]])\n","\n","  # Tokenize text\n","  it_train <- itoken(data_frame[[feature_name]],\n","                     preprocessor = tolower,\n","                     tokenizer = cleaning_tokenizer,\n","                     ids = data_frame$id,\n","                     progressbar = FALSE)\n","\n","  # Create vocabulary\n","  vocab_train <- create_vocabulary(it_train)\n","  if (!is.null(term_count_min) || !is.null(doc_proportion_max)) {\n","    vocab_train <- prune_vocabulary(vocab_train, term_count_min = term_count_min, doc_proportion_max = doc_proportion_max)\n","  }\n","  vocab_small_train <- prune_vocabulary(vocab_train, vocab_term_max = vocab_term_max)\n","  vectorizer_train <- vocab_vectorizer(vocab_small_train)\n","\n","  # Convert to DTM\n","  dtm_train <- create_dtm(it_train, vectorizer_train)\n","  dtm_train_df <- data.frame(as.matrix(dtm_train))\n","\n","  return(list(train_dtm = dtm_train_df))\n","}\n","\n","# Process amenities\n","amenities_result <- process_text_feature(combined_data, \"amenities\", vocab_term_max = 75)\n","\n","# Process features\n","features_result <- process_text_feature(combined_data, \"features\", vocab_term_max = 75)\n","\n","# Process description\n","description_result <- process_text_feature(combined_data, \"description\", vocab_term_max = 75)\n","\n","# Process space\n","space_result <- process_text_feature(combined_data, \"space\", vocab_term_max = 75)\n","\n","# Process summary\n","summary_result <- process_text_feature(combined_data, \"summary\", vocab_term_max = 75)\n","\n","# Process name\n","name_result <- process_text_feature(combined_data, \"name\", vocab_term_max = 75)\n","\n","# Process neighborhood\n","neighborhood_result<- process_text_feature(combined_data, \"neighborhood_overview\", vocab_term_max = 10)\n","\n","# Process access\n","access_result<-process_text_feature(combined_data, \"access\", vocab_term_max = 10)\n","\n","# Process host_about\n","host_about_result<- process_text_feature(combined_data, \"host_about\", vocab_term_max = 10)\n","\n","# Process interaction\n","interaction_result<- process_text_feature(combined_data, \"interaction\", vocab_term_max = 10)\n","\n","# Process notes\n","notes_result<-process_text_feature(combined_data, \"notes\", vocab_term_max = 10)\n","\n","# Process transit\n","transit_result<-process_text_feature(combined_data, \"transit\", vocab_term_max = 10)\n","\n","# Process house_rules\n","house_rules_result<-process_text_feature(combined_data, \"house_rules\", vocab_term_max = 10)\n","\n","# Combine the results\n","combined_cleaned_text_feature <- cbind(amenities_result$train_dtm, features_result$train_dtm,\n","                                       description_result$train_dtm)\n","\n","############################## Bind Text Feature & Remove Duplicates ##############################\n","\n","combined_data_cleaned <- cbind(combined_data_cleaned, combined_cleaned_text_feature)\n","\n","\n","# Identify and remove duplicate column names\n","dup_cols <- which(duplicated(names(combined_data_cleaned)))\n","combined_data_cleaned <- combined_data_cleaned[, -dup_cols]"],"metadata":{"id":"641EecWLe1To"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## External Data for States"],"metadata":{"id":"BDyXF2ZtfAGz"}},{"cell_type":"code","source":["# State Mapping\n","state_mapping <- c(\"Alabama\" = \"AL\",\n","                   \"Alaska\" = \"AK\",\n","                   \"Arizona\" = \"AZ\",\n","                   \"Arkansas\" = \"AR\",\n","                   \"California\" = \"CA\",\n","                   \"Colorado\" = \"CO\",\n","                   \"Connecticut\" = \"CT\",\n","                   \"Delaware\" = \"DE\",\n","                   \"District of Columbia\" = \"DC\",\n","                   \"Florida\" = \"FL\",\n","                   \"Georgia\" = \"GA\",\n","                   \"Hawaii\" = \"HI\",\n","                   \"Idaho\" = \"ID\",\n","                   \"Illinois\" = \"IL\",\n","                   \"Indiana\" = \"IN\",\n","                   \"Iowa\" = \"IA\",\n","                   \"Kansas\" = \"KS\",\n","                   \"Kentucky\" = \"KY\",\n","                   \"Louisiana\" = \"LA\",\n","                   \"Maine\" = \"ME\",\n","                   \"Maryland\" = \"MD\",\n","                   \"Massachusetts\" = \"MA\",\n","                   \"Michigan\" = \"MI\",\n","                   \"Minnesota\" = \"MN\",\n","                   \"Mississippi\" = \"MS\",\n","                   \"Missouri\" = \"MO\",\n","                   \"Montana\" = \"MT\",\n","                   \"Nebraska\" = \"NE\",\n","                   \"Nevada\" = \"NV\",\n","                   \"New Hampshire\" = \"NH\",\n","                   \"New Jersey\" = \"NJ\",\n","                   \"New Mexico\" = \"NM\",\n","                   \"New York\" = \"NY\",\n","                   \"North Carolina\" = \"NC\",\n","                   \"North Dakota\" = \"ND\",\n","                   \"Ohio\" = \"OH\",\n","                   \"Oklahoma\" = \"OK\",\n","                   \"Oregon\" = \"OR\",\n","                   \"Pennsylvania\" = \"PA\",\n","                   \"Rhode Island\" = \"RI\",\n","                   \"South Carolina\" = \"SC\",\n","                   \"South Dakota\" = \"SD\",\n","                   \"Tennessee\" = \"TN\",\n","                   \"Texas\" = \"TX\",\n","                   \"Utah\" = \"UT\",\n","                   \"Vermont\" = \"VT\",\n","                   \"Virginia\" = \"VA\",\n","                   \"Washington\" = \"WA\",\n","                   \"West Virginia\" = \"WV\",\n","                   \"Wisconsin\" = \"WI\",\n","                   \"Wyoming\" = \"WY\")"],"metadata":{"id":"A2kSulWmfGpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################## Happiness Data External ##############################\n","# https://worldpopulationreview.com/state-rankings/happiest-states\n","happiness<- read_csv(\"happiest-states-2024.csv\")\n","happiness$state <- state_mapping[happiness$state]\n","\n","# Identify the top 10 happiest states based on the lowest (best) ranks\n","top_10_states_happ <- happiness %>%\n","  arrange(HappiestStatesCommunityAndEnvironmentRank) %>%  # Ensure this sorts ascending\n","  slice(1:10) %>%\n","  pull(state)  # Use pull to directly extract the state column\n","\n","# Add the new variable to combined_data_cleaned, using as.factor\n","combined_data_cleaned <- combined_data_cleaned %>%\n","  mutate(top10happy = ifelse(state %in% top_10_states_happ, 1, 0),  # Assign 1 for top 10, 0 otherwise\n","         top10happy = as.factor(top10happy))  # Convert to factor"],"metadata":{"id":"BtATJDice_xQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################## Population Data External ##############################\n","# https://worldpopulationreview.com/states\n","\n","population <- read.csv(\"population info.csv\")\n","population$state <- state_mapping[population$state]\n","\n","# Identify the top 10 populous states\n","top_10_states_pop <- population %>%\n","  arrange(desc(pop_2024)) %>%\n","  slice(1:10) %>%\n","  pull(state)\n","\n","# Add the new variable to combined_data_cleaned, using as.factor\n","combined_data_cleaned <- combined_data_cleaned %>%\n","  mutate(top10pop = ifelse(state %in% top_10_states_pop, 1, 0),\n","         top10pop = as.factor(top10pop))  # Convert to factor"],"metadata":{"id":"j3IcnnOle6Dt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################## Weather Data External ##############################\n","# https://worldpopulationreview.com/state-rankings/best-weather-by-state\n","\n","weather <- read.csv(\"best-weather-by-state-2024.csv\")\n","weather$state <- state_mapping[weather$state]\n","\n","# Identify the top 10 states based on the lowest extreme weather events\n","top_10_states_weather <- weather %>%\n","  arrange(StatesWithBestWeatherNumofExremeWeatherEvents) %>%\n","  slice(1:10) %>%\n","  pull(state)\n","\n","# Add the new variable to combined_data_cleaned, using as.factor\n","combined_data_cleaned <- combined_data_cleaned %>%\n","  mutate(top10weather = ifelse(state %in% top_10_states_weather, 1, 0),\n","         top10weather = as.factor(top10weather))  # Convert to factor"],"metadata":{"id":"p4Goem69fPVT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################## Crime Data External ##############################\n","# https://worldpopulationreview.com/state-rankings/best-weather-by-state\n","\n","crime <- read.csv(\"crime-rate-by-state-2024.csv\")\n","crime$state <- state_mapping[crime$state]\n","\n","# View the structure of the data to understand the columns\n","str(crime)\n","\n","# Assuming the crime rate column is named 'CrimeRate', adjust if necessary\n","# Calculate quantiles to define the categories\n","quantiles <- quantile(crime$CrimeRate, probs = c(0, 0.2, 0.4, 0.6, 0.8, 1))\n","\n","# Define the crime category based on quantiles\n","crime$CrimeCategory <- cut(crime$CrimeRate,\n","                           breaks = c(-Inf, quantiles[2], quantiles[3], quantiles[4], quantiles[5], Inf),\n","                           labels = c(\"Very Low Crime\", \"Low Crime\", \"Moderate Crime\", \"High Crime\", \"Very High Crime\"))\n","\n","combined_data_cleaned <- combined_data_cleaned %>%\n","  mutate(crime_rate = ifelse(combined_data_cleaned$state == crime$state, crime$CrimeRate, \"MISSING\"),\n","         crime_rate = as.factor((crime_rate)))"],"metadata":{"id":"QSu8SIOhfS2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Split cleaned_data back into train_x and test_x based on 'dataset' column ###\n","train_x_cleaned <- combined_data_cleaned %>%\n","  filter(dataset == \"train\") %>%\n","  select(-dataset)\n","#summary(train_x_cleaned)\n","\n","test_x_cleaned <- combined_data_cleaned %>%\n","  filter(dataset == \"test\") %>%\n","  select(-dataset)\n","#summary(test_x_cleaned)"],"metadata":{"id":"-Rc71ZVTfWju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################ Join the training y to the training x file ################\n","# turn the target variables into factors\n","train_cleaned <- cbind(train_x_cleaned, train_y) %>%\n","  mutate(perfect_rating_score = as.factor(perfect_rating_score)) %>%\n","  select(-high_booking_rate)"],"metadata":{"id":"adkttiWnfZFV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EDA - Gautam"],"metadata":{"id":"cio0RNHSmBen"}},{"cell_type":"code","source":["#1. market\n","ggplot(train_cleaned, aes(x = market, fill = factor(perfect_rating_score))) +\n","  geom_bar(position = \"fill\") +\n","  scale_y_continuous(labels = scales::percent) +\n","  labs(title = \"Proportion of Perfect Ratings by Market\", x = \"Market\", y = \"Proportion\", fill = \"Perfect Rating Score\") +\n","  theme_minimal() +\n","  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n","\n","#2. host_response\n","ggplot(train_cleaned, aes(x = host_response, fill = factor(perfect_rating_score))) +\n","  geom_bar(position = \"fill\") +\n","  labs(\n","    title = \"Proportion of Perfect Ratings by Host Response\",\n","    x = \"Host Response\",\n","    y = \"Proportion\",\n","    fill = \"Perfect Rating Score\"\n","  ) +\n","  theme_minimal()\n","\n","#3. host_since_days\n","ggplot(train_cleaned, aes(x = as.factor(perfect_rating_score), y = host_since_days, fill = as.factor(perfect_rating_score))) +\n","  geom_boxplot(outlier.color = \"red\") +\n","  labs(title = \"Distribution of Host Since Days by Perfect Rating Score\",\n","       x = \"Perfect Rating Score\",\n","       y = \"Host Since Days\",\n","       fill = \"Perfect Rating Score\")\n","  scale_fill_manual(values = c(\"0\" = \"#FF6666\", \"1\" = \"#66B3FF\"),\n","                    labels = c(\"0 (No)\", \"1 (Yes)\")) +\n","  theme_minimal()\n","\n","\n","#4. summary_length\n","  ggplot(train_cleaned, aes(x = perfect_rating_score, y = summary_length, fill = perfect_rating_score)) +\n","    geom_boxplot(alpha = 0.7) +\n","    labs(title = \"Box Plot of Summary Length by Perfect Rating Score\",\n","         x = \"Perfect Rating Score\",\n","         y = \"Summary Length (Normalized)\",\n","         fill = \"Perfect Rating Score\") +\n","    scale_fill_manual(values = c(\"NO\" = \"#FF6666\", \"YES\" = \"#66B3FF\"),\n","                      labels = c(\"NO\", \"YES\")) +\n","    theme_minimal()\n","\n","#5. monthly_price\n","  ggplot(train_cleaned, aes(x = monthly_price, fill = perfect_rating_score)) +\n","    geom_bar(position = \"dodge\") +\n","    labs(title = \"Monthly Price vs Perfect Rating Score\",\n","         x = \"Monthly Price (0 = Not Available, 1 = Available)\",\n","         y = \"Count\",\n","         fill = \"Perfect Rating Score\") +\n","    scale_fill_manual(values = c(\"NO\" = \"#FF6666\", \"YES\" = \"#66B3FF\"),\n","                      labels = c(\"NO\", \"YES\")) +\n","    theme_minimal()\n","\n","#6. availability_365\n","  ggplot(train_cleaned, aes(x = availability_365, fill = perfect_rating_score)) +\n","    geom_bar(position = \"dodge\") +\n","    labs(title = \"Availability 365 vs Perfect Rating Score\",\n","         x = \"Availability 365 (0 = Not Available, 1 = Available)\",\n","         y = \"Count\",\n","         fill = \"Perfect Rating Score\") +\n","    scale_fill_manual(values = c(\"NO\" = \"#FF6666\", \"YES\" = \"#66B3FF\"),\n","                      labels = c(\"NO\", \"YES\")) +\n","    theme_minimal()\n","\n","#7. description_length\n","  ggplot(train_cleaned, aes(x = description_length, fill = perfect_rating_score)) +\n","    geom_density(alpha = 0.5) +\n","    labs(title = \"Density of Description Length by Perfect Rating Score\",\n","         x = \"Normalized Description Length\",\n","         y = \"Density\",\n","         fill = \"Perfect Rating Score\") +\n","    theme_minimal()\n","\n","#8. cancellation_policy\n","  ggplot(train_cleaned, aes(x = cancellation_policy, fill = perfect_rating_score)) +\n","    geom_bar(position = \"fill\") +\n","    labs(\n","      title = \"Proportion of Perfect Rating Scores by Cancellation Policy\",\n","      x = \"Cancellation Policy\",\n","      y = \"Proportion\",\n","      fill = \"Perfect Rating Score\"\n","    ) +\n","    theme_minimal()\n","\n","#9. log_price\n","  ggplot(train_cleaned, aes(x = log_price, fill = perfect_rating_score)) +\n","    geom_density(alpha = 0.5) +\n","    labs(\n","      title = \"Density Plot of Log Price by Perfect Rating Score\",\n","      x = \"Log Price\",\n","      y = \"Density\",\n","      fill = \"Perfect Rating Score\"\n","    ) +\n","    theme_minimal()\n","\n","#10. log_minimum_nights\n","  ggplot(train_cleaned, aes(x = log_minimum_nights, fill = perfect_rating_score)) +\n","    geom_bar(position = \"dodge\") +\n","    labs(title = \"Effect of Log Minimum Nights on Perfect Rating Score\",\n","         x = \"Log Minimum Nights\",\n","         y = \"Count\",\n","         fill = \"Perfect Rating Score\") +\n","    theme_minimal()"],"metadata":{"id":"igGH7I7DmIJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################ cross validation for model selection ################\n","accuracy <- function(classifications, actuals){\n","  correct_classifications <- ifelse(classifications == actuals, 1, 0)\n","  acc <- sum(correct_classifications)/length(classifications)\n","  return(acc)\n","}\n","\n","set.seed(123)\n","# sample 10% train data\n","market_level <- unique(train_x$market)\n","cv_train_cleaned <- train_cleaned %>%\n","  sample_frac(0.1) %>%\n","  mutate(perfect_rating_score = factor(ifelse(perfect_rating_score == \"YES\", 1, 0)),\n","         market = ifelse(market %in% market_level, market, \"OTHER\"))\n","\n","# shuffle data order\n","k <- 5\n","folds <- cut(seq(1,nrow(cv_train_cleaned)),breaks=k,labels=FALSE)\n","\n","logistic_tpr_folds <- rep(0, k)\n","xbgoost_tpr_folds <- rep(0, k)\n","lasso_tpr_folds <- rep(0, k)\n","ridge_tpr_folds <- rep(0, k)\n","knn_trp_folds<-rep(0,k)\n","randomforest_tpr_folds<-rep(0,k)\n","\n","for (i in 1:k) {\n","  # Define the validation fold indices\n","  valid_inds <- which(folds == i, arr.ind = TRUE)\n","  # Extract training fold\n","  train_fold <- cv_train_cleaned[-valid_inds, ]\n","  train_x_fold <- train_fold %>%\n","    mutate(market = factor(market),\n","           state = factor(state)) %>%\n","    select(-perfect_rating_score)\n","  train_y_fold <- train_fold %>%\n","    select(perfect_rating_score)\n","  market_level_cv <- unique(train_fold$market)\n","\n","  # Extract validation fold\n","  valid_fold <- cv_train_cleaned[valid_inds, ]\n","  valid_fold <- valid_fold %>%\n","    mutate(market = factor(ifelse(market %in% market_level_cv, market, \"OTHER\")),\n","           state = factor(state))\n","  valid_x_fold <- valid_fold %>%\n","    select(-perfect_rating_score)\n","  valid_y_fold <- valid_fold %>%\n","    select(perfect_rating_score)\n","\n","  ### logistic model on training fold\n","  logistic_model_cv <- glm(train_y_fold$perfect_rating_score ~ ., family = 'binomial', data = train_x_fold)\n","  # Predict on validation fold\n","  logistic_pred_on_valid <- predict(logistic_model_cv, newdata = valid_x_fold, type = \"response\")\n","  # Define cutoff for classification\n","  cutoff <- 0.5\n","  # Classify based on cutoff\n","  classification <- factor(ifelse(logistic_pred_on_valid > cutoff, 1, 0), levels = c(\"0\", \"1\"))\n","\n","  # Calculate confusion matrix based on current cutoff\n","  CM_tr <- confusionMatrix(data = classification,\n","                           reference = valid_y_fold$perfect_rating_score,\n","                           positive = \"1\")\n","  # Extract TP and FN from confusion matrix\n","  TP <- CM_tr$table[2, 2]\n","  FN <- CM_tr$table[1, 2]\n","\n","  # Calculate True Positive Rate (TPR)\n","  current_TPR <- TP / (TP + FN)\n","\n","  # Append TPR to logistic_tpr_folds\n","  logistic_tpr_folds[i] <- current_TPR\n","\n","  ### xgboost\n","  train_y_fold <- train_y_fold %>%\n","    mutate(perfect_rating_score = as.numeric(perfect_rating_score),\n","           perfect_rating_score = ifelse(perfect_rating_score == 1, 0, 1))\n","  boost_model <- gbm(train_y_fold$perfect_rating_score~.,data = train_x_fold,\n","                     distribution = \"bernoulli\",\n","                     n.trees=100,\n","                     interaction.depth = 5)\n","  boost_pred <- predict(boost_model,\n","                        newdata = valid_x_fold,\n","                        type = 'response',\n","                        n.trees = 100)\n","  boost_class <- factor(ifelse(boost_pred > cutoff, 1, 0))\n","  # Calculate confusion matrix\n","  CM_tr_cv <- confusionMatrix(data = boost_class,\n","                              reference = valid_y_fold$perfect_rating_score,\n","                              positive = \"1\")\n","  TP <- CM_tr_cv$table[2, 2]\n","  FN <- CM_tr_cv$table[1, 2]\n","  current_TPR <- TP / (TP + FN)\n","  xbgoost_tpr_folds[i] <- current_TPR\n","\n","  ### lasso\n","  lasso_model <- glmnet(as.matrix(train_x_fold), as.matrix(train_y_fold), family = \"binomial\", alpha = 1, lambda = 0.0004822898)\n","  lasso_pred_probabilities <- predict(lasso_model, newx = as.matrix(valid_x_fold), s = \"lambda.min\", type = \"response\")\n","  lasso_class <- factor(ifelse(lasso_pred_probabilities > cutoff, 1, 0), levels = c(\"0\", \"1\"))\n","  CM_tr_lasso <- confusionMatrix(data = lasso_class,\n","                                 reference = valid_y_fold$perfect_rating_score,\n","                                 positive = \"1\")\n","  TP <- CM_tr_lasso$table[2, 2]\n","  FN <- CM_tr_lasso$table[1, 2]\n","  current_TPR <- TP / (TP + FN)\n","  lasso_tpr_folds[i] <- current_TPR\n","\n","  ### ridge\n","  ridge_model <- glmnet(as.matrix(train_x_fold), as.matrix(train_y_fold), family = \"binomial\", alpha = 0, lambda = 0.001)\n","  ridge_pred_probabilities <- predict(ridge_model, newx = as.matrix(valid_x_fold), s = \"lambda.min\", type = \"response\")\n","  ridge_class <- factor(ifelse(ridge_pred_probabilities > cutoff, 1, 0), levels = c(\"0\", \"1\"))\n","  CM_tr_ridge <- confusionMatrix(data = ridge_class,\n","                                 reference = valid_y_fold$perfect_rating_score,\n","                                 positive = \"1\")\n","  TP <- CM_tr_ridge$table[2, 2]\n","  FN <- CM_tr_ridge$table[1, 2]\n","  current_TPR <- TP / (TP + FN)\n","  ridge_tpr_folds[i] <- current_TPR\n","\n","  ### Random Forest\n","\n","  rf_model_cv <- randomForest(train_y_fold$perfect_rating_score ~ ., data = train_x_fold, ntree = 500, mtry = sqrt(ncol(train_x_fold)))\n","  rf_pred_cv <- predict(rf_model_cv, newdata = valid_x_fold, type = \"response\")\n","  rf_class <- factor(ifelse(rf_pred_cv > cutoff, 1, 0))\n","  CM_tr_rf <- confusionMatrix(data = rf_class,\n","                              reference = valid_y_fold$perfect_rating_score,\n","                              positive = \"1\")\n","\n","  TP <- CM_tr_rf$table[2, 2]\n","  FN <- CM_tr_rf$table[1, 2]\n","  current_TPR <- TP / (TP + FN)\n","  randomforest_tpr_folds[i] <- current_TPR\n","\n","  ### kNN\n","\n","  knn_model_cv <- knn(train_x_fold, valid_x_fold, train_y_fold$perfect_rating_score, k = best_k)\n","  knn_pred_cv <- as.factor(knn_model_cv)\n","  CM_tr_knn <- confusionMatrix(data = knn_pred_cv,\n","                                reference = valid_y_fold$perfect_rating_score,\n","                                positive = \"1\")\n","\n","  TP <- CM_tr_knn$table[2, 2]\n","  FN <- CM_tr_knn$table[1, 2]\n","  current_TPR <- TP / (TP + FN)\n","  knn_tpr_folds[i] <- current_TPR\n","\n","}\n","\n","cat(\"Average TPR on 5 fold-CV:\", mean(logistic_tpr_folds), \"\\n\")\n","cat(\"Average TPR on 5 fold-CV:\", mean(xbgoost_tpr_folds), \"\\n\")\n","cat(\"Average TPR on 5 fold-CV:\", mean(lasso_tpr_folds), \"\\n\")\n","cat(\"Average TPR on 5 fold-CV:\", mean(ridge_tpr_folds), \"\\n\")\n","cat(\"Average TPR on 5 fold-CV:\", mean(randomforest_tpr_folds), \"\\n\")\n","cat(\"Average TPR on 5 fold-CV:\", mean(knn_tpr_folds), \"\\n\")\n","\n","# Combine the TPR vectors and create a data frame\n","k_fold <- c(1,2,3,4,5)\n","tpr_data <- data.frame(k_fold, logistic_tpr_folds, xbgoost_tpr_folds, lasso_tpr_folds, ridge_tpr_folds,knn_tpr_folds, randomforest_tpr_folds)\n","# Set the outer margin to make room for the legend\n","# Plotting\n","barplot(\n","  height = t(tpr_data[, -1]),  # Extracting TPR columns and transposing\n","  beside = TRUE,\n","  ylim = c(0, 0.5),\n","  main = \"Comparison of TPR for Different Models across k_fold\",\n","  names.arg = tpr_data$k_fold,\n","  xlab = \"k_fold\",\n","  ylab = \"True Positive Rate\",\n","  col = c(\"blue\", \"red\", \"orange\", \"grey\",\"green\",\"yellow\"),\n","  legend.text = c(\"Logistic\", \"XGBoost\", \"Lasso\", \"Ridge\", \"KNN\",\"Random Forest\")\n",")\n","abline(h = mean(logistic_tpr_folds), col = \"blue\", lty = \"dashed\")\n","abline(h = mean(xbgoost_tpr_folds), col = \"red\", lty = \"dashed\")\n","abline(h = mean(lasso_tpr_folds), col = \"orange\", lty = \"dashed\")\n","abline(h = mean(ridge_tpr_folds), col = \"grey\", lty = \"dashed\")\n","abline(h = mean(knn_tpr_folds), col = \"green\", lty = \"dashed\")\n","abline(h = mean(randomforest_tpr_folds), col = \"yellow\", lty = \"dashed\")"],"metadata":{"id":"vLrSQ6HulfFv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Logistic Model - Jerry"],"metadata":{"id":"fnwlfcjjlo-v"}},{"cell_type":"code","source":["train_inst <- sample(nrow(train_cleaned), 0.70*nrow(train_cleaned))\n","train_x <- train_cleaned[train_inst,]\n","train_y <- train_x %>%\n","  select(perfect_rating_score) %>%\n","  mutate(perfect_rating_score = as.factor(perfect_rating_score))\n","train_x <- train_x %>%\n","  select(-perfect_rating_score) %>%\n","  mutate(state = factor(state))\n","market_level <- unique(train_x$market)\n","\n","valid_x <- train_cleaned[-train_inst,]\n","valid_y <- valid_x %>%\n","  select(perfect_rating_score) %>%\n","  mutate(perfect_rating_score = as.factor(perfect_rating_score))\n","valid_x <- valid_x %>%\n","  select(-perfect_rating_score) %>%\n","  mutate(market = case_when(\n","    market %in% market_level ~ market,\n","    TRUE ~ \"OTHER\",\n","\n","  ),\n","  state = factor(state))\n","\n","cutoffs <- seq(0.01, 1.0, by = 0.01)\n","val_accs <- rep(0, length(cutoffs))\n","optimal_cutoff <- NULL\n","max_TPR <- 0\n","corresponding_FPR <- NULL\n","logistic_tpr <- rep(0, length(cutoffs))\n","logistic_fpr <- rep(0, length(cutoffs))\n","\n","# Iterate through different cutoff values\n","logistic_model <- glm(train_y$perfect_rating_score~., family = 'binomial', data = train_x)\n","logistic_pred_on_valid <- predict(logistic_model, newdata = valid_x, type = \"response\")\n","\n","for (i in c(1:length(cutoffs))) {\n","  cutoff = cutoffs[i]\n","  # Convert predicted probabilities to classification based on cutoff\n","  classification <- factor(ifelse(logistic_pred_on_valid > cutoff, \"YES\", \"NO\"),\n","                           levels = levels(train_y$perfect_rating_score))\n","  val_accs[i] <- accuracy(classification, valid_y$perfect_rating_score)\n","\n","  # Calculate confusion matrix based on current cutoff\n","  CM_tr <- confusionMatrix(data = classification,\n","                           reference = valid_y$perfect_rating_score,\n","                           positive = \"YES\")\n","\n","  # Extract TP, FN, TN, FP from confusion matrix\n","  TP <- CM_tr$table[\"YES\", \"YES\"]\n","  FN <- CM_tr$table[\"NO\", \"YES\"]\n","  TN <- CM_tr$table[\"NO\", \"NO\"]\n","  FP <- CM_tr$table[\"YES\", \"NO\"]\n","\n","  # Calculate True Positive Rate (TPR) and False Positive Rate (FPR)\n","  current_TPR <- TP / (TP + FN)\n","  current_FPR <- FP / (TN + FP)\n","  logistic_tpr[i] <- current_TPR\n","  logistic_fpr[i] <- current_FPR\n","\n","  # Check if current FPR is less than 10% and update optimal cutoff if TPR is higher\n","  if (current_FPR <= 0.1 && current_TPR > max_TPR) {\n","    optimal_cutoff <- cutoff\n","    max_TPR <- current_TPR\n","    corresponding_FPR <- current_FPR\n","  }\n","}\n","\n","# Print optimal cutoff and corresponding TPR, FPR\n","cat(\"Optimal Cutoff:\", optimal_cutoff, \"\\n\")\n","cat(\"Corresponding TPR:\", max_TPR, \"\\n\")\n","cat(\"Corresponding FPR:\", corresponding_FPR, \"\\n\")\n","\n","# train TPR accuracy\n","logistic_pred_on_train <- predict(logistic_model, newdata = train_x, type = \"response\")\n","classification <- factor(ifelse(logistic_pred_on_train > optimal_cutoff, 1, 0), levels = levels(valid_y$perfect_rating_score))\n","CM_tr <- confusionMatrix(data = classification,\n","                         reference = train_y$perfect_rating_score,\n","                         positive = \"1\")\n","CM_tr$table\n","TP <- CM_tr$table[\"1\", \"1\"]\n","FN <- CM_tr$table[\"0\", \"1\"]\n","TN <- CM_tr$table[\"0\", \"0\"]\n","FP <- CM_tr$table[\"1\", \"0\"]\n","\n","TR_TPR <- TP / (TP + FN)\n","TR_FPR <- FP / (TN + FP)\n","TR_TPR\n","TR_FPR\n","\n","logistic_roc <- data.frame(logistic_tpr, logistic_fpr)\n","\n","################################# use all train data to prediction on test data\n","################################# use all train data to prediction on test data\n","################################# use all train data to prediction on test data\n","\n","logistic_model <- glm(perfect_rating_score~., family = 'binomial', data = train_x_cleaned)\n","logistic_pred_on_test <- predict(logistic_model, newdata = test_x_cleaned, type = \"response\")\n","classifications_perfect <- factor(ifelse(logistic_pred_on_test > optimal_cutoff, \"YES\", \"NO\"))\n","assertthat::assert_that(sum(is.na(classifications_perfect))==0)\n","table(classifications_perfect)"],"metadata":{"id":"3ynPDMMYlt9M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ridge Model - Pravah"],"metadata":{"id":"pxDwbFtofbl1"}},{"cell_type":"code","source":["############################ Run train validation on full model ###############################\n","# Set seed for reproducibility\n","set.seed(123)\n","\n","# Sample indices for creating training and validation sets\n","train_indices <- sample(nrow(train_cleaned), 0.70 * nrow(train_cleaned))\n","train_data <- train_cleaned[train_indices, ]\n","valid_data <- train_cleaned[-train_indices, ]\n","\n","# Ensure all 'market' levels present in validation are in training\n","market_level <- unique(train_data$market)\n","valid_data <- valid_data %>%\n","  mutate(market = case_when(\n","    market %in% market_level ~ market,\n","    TRUE ~ \"OTHER\"\n","  ))\n","\n","# Create the full model matrix from the adjusted data, then split\n","full_data <- bind_rows(train_data, valid_data) # Bind rows to keep structure\n","full_matrix <- model.matrix(~ . - 1 - perfect_rating_score, data = full_data)\n","\n","# Number of train data rows may change due to filtering, recalculate it\n","train_n <- nrow(train_data)\n","\n","# Split the full matrix into training and validation matrices\n","x_train <- full_matrix[1:train_n, ]\n","x_valid <- full_matrix[(train_n + 1):nrow(full_matrix), ]\n","\n","# Prepare the response variable\n","y_full <- as.numeric(as.factor(full_data$perfect_rating_score)) - 1\n","\n","# Split the response variable into training and validation\n","y_train <- y_full[1:train_n]\n","y_valid <- y_full[(train_n + 1):nrow(full_matrix)]\n","\n","# Fit the model using cross-validation to find the best lambda\n","cv_model <- cv.glmnet(x_train, y_train, family = \"binomial\", alpha = 0)\n","plot(cv_model)\n","\n","# Extract the best lambda and fit the final model\n","best_lambda <- cv_model$lambda.min\n","ridge_model <- glmnet(x_train, y_train, family = \"binomial\", alpha = 0, lambda = best_lambda)\n","\n","# Predict probabilities using the validation data\n","ridge_pred_probabilities <- predict(ridge_model, newx = x_valid, s = \"lambda.min\", type = \"response\")\n","\n","# Initialize variables to store optimal cutoff and performance metrics\n","optimal_cutoff <- 0\n","max_TPR <- 0\n","corresponding_FPR <- 0\n","\n","# Define range of cutoff values for the ridge model\n","cutoff_range_ridge <- seq(0.001, 0.999, by = 0.001)\n","\n","# Nested loop to iterate through all cutoff values\n","for (cutoff_ridge in cutoff_range_ridge) {\n","  # Make predictions for the ridge model using the current cutoff value\n","  predictions_ridge <- ifelse(ridge_pred_probabilities > cutoff_ridge, \"YES\", \"NO\")\n","\n","  # Calculate confusion matrix based on ridge model predictions\n","  CM_ridge <- confusionMatrix(data = factor(predictions_ridge, levels = c(\"YES\", \"NO\")),\n","                              reference = as.factor(valid_data$perfect_rating_score),  # Use valid_data for reference\n","                              positive = \"YES\")\n","\n","  # Extract TP, FN, TN, FP from confusion matrix\n","  TP <- CM_ridge$table[\"YES\", \"YES\"]\n","  FN <- CM_ridge$table[\"NO\", \"YES\"]\n","  TN <- CM_ridge$table[\"NO\", \"NO\"]\n","  FP <- CM_ridge$table[\"YES\", \"NO\"]\n","\n","  # Calculate True Positive Rate (TPR) and False Positive Rate (FPR)\n","  current_TPR <- TP / (TP + FN)\n","  current_FPR <- FP / (TN + FP)\n","\n","  # Check if current FPR is less than 10% and update optimal cutoff if TPR is higher\n","  if (current_FPR < 0.095 && current_TPR > max_TPR) {\n","    max_TPR <- current_TPR\n","    corresponding_FPR <- current_FPR\n","    optimal_cutoff <- cutoff_ridge\n","  }\n","}\n","\n","# Print optimal cutoff and performance metrics for train-valid split\n","print(\"Train-Valid Split Model:\")\n","print(paste(\"Optimal Cutoff for ridge Model:\", optimal_cutoff))\n","print(paste(\"Max TPR with FPR < 10%:\", max_TPR))\n","print(paste(\"Corresponding FPR:\", corresponding_FPR))"],"metadata":{"id":"GHI_6RCJfbW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################# Check if external data variables improve model: #################################\n","# Exclude 'top10happy' from the model matrix\n","full_matrix_without <- model.matrix(~ . - 1 - perfect_rating_score - top10happy - top10pop - crime_rate - top10weather, data = full_data)\n","\n","# Split the response variable into training and validation as before\n","x_train_without <- full_matrix_without[1:train_n, ]\n","x_valid_without <- full_matrix_without[(train_n + 1):nrow(full_matrix_without), ]\n","\n","# Fit the model without 'top10happy'\n","cv_model_without <- cv.glmnet(x_train_without, y_train, family = \"binomial\", alpha = 0)\n","best_lambda_without <- cv_model_without$lambda.min\n","ridge_model_without <- glmnet(x_train_without, y_train, family = \"binomial\", alpha = 0, lambda = best_lambda_without)\n","\n","# Predict probabilities using the validation data for the model without 'top10happy'\n","ridge_pred_probabilities_without <- predict(ridge_model_without, newx = x_valid_without, s = \"lambda.min\", type = \"response\")\n","\n","# Calculate performance metrics at the same optimal cutoff determined previously\n","predictions_ridge_without <- ifelse(ridge_pred_probabilities_without > optimal_cutoff, \"YES\", \"NO\")\n","CM_ridge_without <- confusionMatrix(data = factor(predictions_ridge_without, levels = c(\"YES\", \"NO\")),\n","                                    reference = as.factor(valid_data$perfect_rating_score),\n","                                    positive = \"YES\")\n","\n","# Extract TP, FN, TN, FP from confusion matrix for the model without 'top10happy'\n","TP_without <- CM_ridge_without$table[\"YES\", \"YES\"]\n","FN_without <- CM_ridge_without$table[\"NO\", \"YES\"]\n","TN_without <- CM_ridge_without$table[\"NO\", \"NO\"]\n","FP_without <- CM_ridge_without$table[\"YES\", \"NO\"]\n","\n","# Calculate True Positive Rate (TPR) and False Positive Rate (FPR) for the model without external\n","TPR_without <- TP_without / (TP_without + FN_without)\n","FPR_without <- FP_without / (TN_without + FP_without)\n","\n","# Print results for comparison\n","print(\"Model Comparison at Optimal Cutoff:\")\n","print(paste(\"Optimal Cutoff Used:\", optimal_cutoff))\n","print(\"With external:\")\n","print(paste(\"TPR:\", max_TPR, \"FPR:\", corresponding_FPR))\n","print(\"Without external:\")\n","print(paste(\"TPR:\", TPR_without, \"FPR:\", FPR_without))"],"metadata":{"id":"vta-85_Ufnsg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################# Predict on full train data #################################\n","# Ensure all 'market' levels present in validation are in training\n","market_level_full <- unique(train_cleaned$market)\n","test_x_cleaned<- test_x_cleaned %>%\n","  mutate(market = case_when(\n","    market %in% market_level_full ~ market,\n","    TRUE ~ \"OTHER\"\n","  ))\n","\n","train_cleaned_test<-train_cleaned%>%\n","  select(-perfect_rating_score)\n","\n","# Create model matrices\n","full_data_test <- bind_rows(train_cleaned_test, test_x_cleaned)\n","full_matrix_test <- model.matrix(~ . - 1, data = full_data_test) # Now not excluding 'perfect_rating_score' here because it's already removed\n","\n","# Define indices for train and test data within the combined matrix\n","train_n_full <- nrow(train_cleaned_test)\n","x_train_test <- full_matrix_test[1:train_n_full, ]\n","x_test <- full_matrix_test[(train_n_full + 1):nrow(full_matrix_test), ]\n","\n","# Prepare the response variable, ensuring it's correctly specified\n","y_train_test <- as.numeric(train_cleaned$perfect_rating_score) - 1 # Assuming 'perfect_rating_score' is still a factor\n","\n","# Re-train the model on the full training data\n","cv_model_full <- cv.glmnet(x_train_test, y_train_test, family = \"binomial\", alpha = 0)\n","best_lambda_full <- cv_model_full$lambda.min\n","ridge_model_full <- glmnet(x_train_test, y_train_test, family = \"binomial\", alpha = 0, lambda = best_lambda_full)\n","\n","# Predict on the test data\n","ridge_pred_on_test <- predict(ridge_model_full, newx = x_test, s = \"lambda.min\", type = \"response\")\n","\n","# Convert probabilities to factor based on optimal cutoff\n","classifications_perfect <- factor(ifelse(ridge_pred_on_test > optimal_cutoff, \"YES\", \"NO\"))\n","\n","# Check for missing values\n","assertthat::assert_that(sum(is.na(classifications_perfect)) == 0)\n","\n","# View the distribution of predicted classes\n","table(classifications_perfect)"],"metadata":{"id":"0LrcWG3Sfw4m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## XGboost Model - Jerry"],"metadata":{"id":"02b-dCYsmKrO"}},{"cell_type":"code","source":["train_y <- train_y %>%\n","  mutate(perfect_rating_score = ifelse(perfect_rating_score == 'NO', 0, 1))\n","valid_y <- valid_y %>%\n","  mutate(perfect_rating_score = ifelse(perfect_rating_score == 'NO', 0, 1))\n","\n","boost.mod <- gbm(train_y$perfect_rating_score~.,data=train_x,\n","                 distribution=\"bernoulli\",\n","                 n.trees=150,\n","                 interaction.depth=10)\n","boost_preds <- predict(boost.mod,\n","                       newdata=valid_x,\n","                       type='response',\n","                       n.trees=150)\n","xgboost_max_TPR <- 0\n","xgboost_FPR <- 0\n","xgboost_best_cutoff <- 0\n","xgboost_tpr <- rep(0, length(cutoffs))\n","xgboost_fpr <- rep(0, length(cutoffs))\n","\n","\n","for (i in c(1:length(cutoffs))) {\n","  cutoff <- cutoffs[i]\n","  # Convert predicted classes to factor with same levels as reference\n","  boost_class <- ifelse(boost_preds > cutoff, 1, 0)\n","  valid_y <- valid_y %>%\n","    mutate(perfect_rating_score = factor(perfect_rating_score))\n","  boost_class_factor <- factor(boost_class, levels = levels(valid_y$perfect_rating_score))\n","\n","  # Calculate confusion matrix\n","  CM_tr <- confusionMatrix(data = boost_class_factor,\n","                           reference = valid_y$perfect_rating_score,\n","                           positive = \"1\")\n","\n","  # Extract TP, FN, TN, FP from confusion matrix\n","  TP <- CM_tr$table[\"1\", \"1\"]\n","  FN <- CM_tr$table[\"0\", \"1\"]\n","  TN <- CM_tr$table[\"0\", \"0\"]\n","  FP <- CM_tr$table[\"1\", \"0\"]\n","\n","  # Calculate True Positive Rate (TPR) and False Positive Rate (FPR)\n","  current_TPR <- TP / (TP + FN)\n","  current_FPR <- FP / (TN + FP)\n","  xgboost_tpr[i] <- current_TPR\n","  xgboost_fpr[i] <- current_FPR\n","\n","  # Check if current cutoff produces higher TPR with FPR < 10%\n","  if (current_TPR > xgboost_max_TPR & current_FPR < 0.092) {\n","    xgboost_max_TPR <- current_TPR\n","    xgboost_best_cutoff <- cutoff\n","    xgboost_FPR <- current_FPR  # Capture the corresponding FPR\n","  }\n","}\n","\n","cat(\"Optimal Cutoff:\", xgboost_best_cutoff, \"\\n\")\n","cat(\"Corresponding TPR:\", xgboost_max_TPR, \"\\n\")\n","cat(\"Corresponding FPR:\", xgboost_FPR, \"\\n\")\n","\n","# train TPR accuracy\n","boost_preds_tr <- predict(boost.mod,\n","                       newdata=train_x,\n","                       type='response',\n","                       n.trees=150)\n","boost_class_tr <- factor(ifelse(boost_preds_tr > xgboost_best_cutoff, 1, 0))\n","train_y <- train_y %>%\n","  mutate(perfect_rating_score = factor(perfect_rating_score))\n","CM_tr <- confusionMatrix(data = boost_class_tr,\n","                         reference = train_y$perfect_rating_score,\n","                         positive = \"1\")\n","TP <- CM_tr$table[\"1\", \"1\"]\n","FN <- CM_tr$table[\"0\", \"1\"]\n","TN <- CM_tr$table[\"0\", \"0\"]\n","FP <- CM_tr$table[\"1\", \"0\"]\n","TR_TPR <- TP / (TP + FN)\n","TR_FPR <- FP / (TN + FP)\n","TR_TPR\n","TR_FPR\n","\n","xgboost_roc <- data.frame(xgboost_tpr, xgboost_fpr)\n","\n","################################# use all train data to prediction on test data\n","################################# use all train data to prediction on test data\n","################################# use all train data to prediction on test data\n","train_x_feature <- train_x_cleaned %>%\n","  select(-perfect_rating_score)\n","train_x_label <- train_x_cleaned %>%\n","  select(perfect_rating_score) %>%\n","  mutate(perfect_rating_score = ifelse(perfect_rating_score == \"YES\", 1, 0))\n","\n","\n","boost.mod <- gbm(train_x_label$perfect_rating_score~.,data=train_x_feature,\n","                 distribution=\"bernoulli\",\n","                 n.trees=150,\n","                 interaction.depth=10)\n","\n","boost_preds_on_test <- predict(boost.mod,\n","                               newdata=test_x_cleaned,\n","                               type='response',\n","                               n.trees=150)\n","\n","classifications_perfect_boost <- ifelse(boost_preds_on_test>0.4665, \"YES\", \"NO\")\n","assertthat::assert_that(sum(is.na(classifications_perfect_boost))==0)\n","table(classifications_perfect_boost)\n","\n","write.table(classifications_perfect_boost, \"perfect_rating_score_group11_boost_final.csv\", row.names = FALSE)"],"metadata":{"id":"jKtQXPnimOL5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Lasso Model - Wendy"],"metadata":{"id":"KeGY-Q7IXQxu"}},{"cell_type":"code","source":["# Set seed for reproducibility\n","set.seed(123)\n","\n","# Sample indices for creating training and validation sets\n","train_indices <- sample(nrow(train_cleaned), 0.70 * nrow(train_cleaned))\n","train_data <- train_cleaned[train_indices, ]\n","valid_data <- train_cleaned[-train_indices, ]\n","\n","# Ensure all 'market' levels present in validation are in training\n","market_level <- unique(train_data$market)\n","valid_data <- valid_data %>%\n","  mutate(market = case_when(\n","    market %in% market_level ~ market,\n","    TRUE ~ \"OTHER\"\n","  ))\n","\n","# Create the full model matrix from the adjusted data, then split\n","full_data <- bind_rows(train_data, valid_data) # Bind rows to keep structure\n","full_matrix <- model.matrix(~ . - 1 - perfect_rating_score, data = full_data)\n","\n","# Number of train data rows may change due to filtering, recalculate it\n","train_n <- nrow(train_data)\n","\n","# Split the full matrix into training and validation matrices\n","x_train <- full_matrix[1:train_n, ]\n","x_valid <- full_matrix[(train_n + 1):nrow(full_matrix), ]\n","\n","# Prepare the response variable\n","y_full <- as.numeric(as.factor(full_data$perfect_rating_score)) - 1\n","\n","# Split the response variable into training and validation\n","y_train <- y_full[1:train_n]\n","y_valid <- y_full[(train_n + 1):nrow(full_matrix)]\n","\n","# Fit the model using cross-validation to find the best lambda\n","cv_model <- cv.glmnet(x_train, y_train, family = \"binomial\", alpha = 1)\n","# Extract the lambda values and corresponding cross-validated errors\n","lambda_values <- cv_model$lambda\n","cv_errors <- cv_model$cvm\n","\n","# Plotting model complexity vs. error\n","plot_data <- data.frame(Lambda = lambda_values, Error = cv_errors)\n","\n","ggplot(plot_data, aes(x = log(Lambda), y = Error)) +\n","  geom_line() +\n","  geom_point(aes(color = Lambda), size = 2) +\n","  scale_color_continuous(trans = 'reverse') +\n","  labs(x = \"Log(Lambda) - Model Complexity\", y = \"Cross-Validated Error\",\n","       title = \"Error vs. Model Complexity for Lasso Model\") +\n","  theme_minimal()\n","\n","# Extract the best lambda and fit the final model\n","best_lambda <- cv_model$lambda.min\n","lasso_model <- glmnet(x_train, y_train, family = \"binomial\", alpha = 1, lambda = best_lambda)\n","\n","# Predict probabilities using the validation data\n","lasso_pred_probabilities <- predict(lasso_model, newx = x_valid, s = \"lambda.min\", type = \"response\")\n","\n","# Initialize variables to store optimal cutoff and performance metrics\n","optimal_cutoff <- 0\n","max_TPR <- 0\n","corresponding_FPR <- 0\n","\n","# Define range of cutoff values for the Lasso model\n","cutoff_range_lasso <- seq(0.001, 0.999, by = 0.001)\n","\n","# Nested loop to iterate through all cutoff values\n","for (cutoff_lasso in cutoff_range_lasso) {\n","  # Make predictions for the Lasso model using the current cutoff value\n","  predictions_lasso <- ifelse(lasso_pred_probabilities > cutoff_lasso, \"YES\", \"NO\")\n","\n","  # Calculate confusion matrix based on Lasso model predictions\n","  CM_lasso <- confusionMatrix(data = factor(predictions_lasso, levels = c(\"YES\", \"NO\")),\n","                              reference = as.factor(valid_data$perfect_rating_score),  # Use valid_data for reference\n","                              positive = \"YES\")\n","\n","  # Extract TP, FN, TN, FP from confusion matrix\n","  TP <- CM_lasso$table[\"YES\", \"YES\"]\n","  FN <- CM_lasso$table[\"NO\", \"YES\"]\n","  TN <- CM_lasso$table[\"NO\", \"NO\"]\n","  FP <- CM_lasso$table[\"YES\", \"NO\"]\n","\n","  # Calculate True Positive Rate (TPR) and False Positive Rate (FPR)\n","  current_TPR <- TP / (TP + FN)\n","  current_FPR <- FP / (TN + FP)\n","\n","  # Check if current FPR is less than 10% and update optimal cutoff if TPR is higher\n","  if (current_FPR < 0.095 && current_TPR > max_TPR) {\n","    max_TPR <- current_TPR\n","    corresponding_FPR <- current_FPR\n","    optimal_cutoff <- cutoff_lasso\n","  }\n","}\n","\n","# Print optimal cutoff and performance metrics for train-valid split\n","print(\"Train-Valid Split Model:\")\n","print(paste(\"Optimal Cutoff for Lasso Model:\", optimal_cutoff))\n","print(paste(\"Max TPR with FPR < 10%:\", max_TPR))\n","print(paste(\"Corresponding FPR:\", corresponding_FPR))"],"metadata":{"id":"0C1DYFqsXQUd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Random Forest - Navya"],"metadata":{"id":"ROzYQt3M4AS1"}},{"cell_type":"code","source":["# Data Splitting\n","train_inst <- sample(nrow(train_x_cleaned), 0.70 * nrow(train_x_cleaned))\n","train_x <- train_x_cleaned[train_inst, ]\n","train_y <- ifelse(train_x$perfect_rating_score == \"NO\", 0, 1)\n","train_x <- train_x[, -names(train_x) %in% c(\"perfect_rating_score\")]\n","\n","valid_x <- train_x_cleaned[-train_inst, ]\n","valid_y <- ifelse(valid_x$perfect_rating_score == \"NO\", 0, 1)\n","valid_x <- valid_x[, -names(valid_x) %in% c(\"perfect_rating_score\")]\n","\n","# Train Random Forest\n","rf_model <- randomForest(train_y ~ ., data = train_x, ntree = 500, mtry = sqrt(ncol(train_x)))\n","\n","# Make predictions on validation set\n","rf_pred <- predict(rf_model, newdata = valid_x, type = \"response\")\n","\n","# Initialize variables for optimal cutoff and corresponding TPR and FPR\n","optimal_cutoff <- NULL\n","max_TPR <- 0\n","corresponding_FPR <- NULL\n","\n","# Loop to find optimal cutoff\n","for (cutoff in seq(0.01, 0.99, by = 0.01)) {\n","  # Make predictions on validation set\n","  rf_pred <- predict(rf_model, newdata = valid_x, type = \"prob\")[, 2] > cutoff\n","\n","  # Calculate True Positive Rate (TPR) and False Positive Rate (FPR)\n","  conf_matrix <- table(rf_pred, valid_y)\n","  TN <- conf_matrix[1, 1]\n","  FP <- conf_matrix[1, 2]\n","  FN <- conf_matrix[2, 1]\n","  TP <- conf_matrix[2, 2]\n","  FPR <- FP / (FP + TN)\n","  TPR <- TP / (TP + FN)\n","\n","  # Check if current FPR is less than 9.5% and update optimal cutoff if TPR is higher\n","  if (FPR < 0.095 && TPR > max_TPR) {\n","    max_TPR <- TPR\n","    corresponding_FPR <- FPR\n","    optimal_cutoff <- cutoff\n","  }\n","}\n","\n","# Output optimal cutoff and corresponding TPR and FPR\n","cat(\"Optimal Cutoff:\", optimal_cutoff, \"\\n\")\n","cat(\"Corresponding TPR:\", max_TPR, \"\\n\")\n","cat(\"Corresponding FPR:\", corresponding_FPR, \"\\n\")"],"metadata":{"id":"xwadYqJu4GMU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## KNN Model - Navya"],"metadata":{"id":"Hvu-x0_B4Gor"}},{"cell_type":"code","source":["# Data Splitting\n","train_inst <- sample(nrow(train_x_cleaned), 0.70 * nrow(train_x_cleaned))\n","train_x <- train_x_cleaned[train_inst, ]\n","train_y <- train_x$perfect_rating_score\n","train_x <- train_x[, -which(names(train_x) == \"perfect_rating_score\")]\n","\n","valid_x <- train_x_cleaned[-train_inst, ]\n","valid_y <- valid_x$perfect_rating_score\n","valid_x <- valid_x[, -which(names(valid_x) == \"perfect_rating_score\")]\n","\n","# Target Variable Transformation\n","train_y <- ifelse(train_y == \"NO\", 0, 1)\n","valid_y <- ifelse(valid_y == \"NO\", 0, 1)\n","\n","# Define evaluate_knn function\n","evaluate_knn <- function(train_x, train_y, valid_x, valid_y, k) {\n","  knn_model <- knn(train_x, valid_x, train_y, k = k)\n","  confusion_matrix <- table(knn_model, valid_y)\n","  TN <- confusion_matrix[1, 1]\n","  FP <- confusion_matrix[1, 2]\n","  FN <- confusion_matrix[2, 1]\n","  TP <- confusion_matrix[2, 2]\n","  FPR <- FP / (FP + TN)  # False Positive Rate\n","  TPR <- TP / (TP + FN)  # True Positive Rate\n","  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)  # Accuracy\n","  return(list(FPR = FPR, TPR = TPR, accuracy = accuracy))\n","}\n","\n","# KNN Model Training\n","\n","best_k <- NULL\n","best_FPR <- 1\n","\n","for (k in 1:ncol(train_x)) {\n","  evaluation <- evaluate_knn(train_x, train_y, valid_x, valid_y, k)\n","  FPR <- evaluation$FPR\n","  TPR <- evaluation$TPR\n","  accuracy <- evaluation$accuracy\n","\n","  if (FPR < 0.095) {\n","    if (FPR < best_FPR) {\n","      best_k <- k\n","      best_FPR <- FPR\n","      best_TPR <- TPR\n","      best_accuracy <- accuracy\n","    }\n","  }\n","}\n","\n","cat(\"Best k:\", best_k, \"\\n\")\n","cat(\"Corresponding FPR:\", best_FPR, \"\\n\")\n","cat(\"Corresponding TPR:\", best_TPR, \"\\n\")\n","cat(\"Accuracy:\", best_accuracy, \"\\n\")\n","\n","# Train KNN with the best k\n","knn_model <- knn(train_x, valid_x, train_y, k = best_k)\n","\n","#Predict with the best knn model\n","knn_pred <- predict(knn_model, valid_x)"],"metadata":{"id":"K3J-BwvY_25C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TPR/ FPR curve\n","plot(xgboost_fpr, xgboost_tpr, type = \"l\",\n","     xlab = \"False Positive Rate (FPR)\",\n","     ylab = \"True Positive Rate (TPR)\",\n","     col = \"lightblue\",\n","     lwd = 2,\n","     main = \"ROC Curve for XGBoost\")\n","abline(a = 0, b = 1, col = \"grey4\", lty = 2)\n","grid(col = \"grey\", lty = \"dotted\")"],"metadata":{"id":"CWfxVQsOONUj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ridge and Lasso Learning Curve at 0.5 Cutoff"],"metadata":{"id":"6uU2nsfSsCU4"}},{"cell_type":"code","source":["accuracy <- function(classifications, actuals){\n","  correct_classifications <- ifelse(classifications == actuals, 1, 0)\n","  acc <- sum(correct_classifications)/length(classifications)\n","  return(acc)\n","}\n","set.seed(123)\n","\n","# Split the initial dataset into 70% train and 30% validation\n","initial_train_indices <- createDataPartition(y = train_cleaned$perfect_rating_score, p = 0.7, list = FALSE)\n","train_data_initial <- train_cleaned[initial_train_indices, ]\n","valid_data <- train_cleaned[-initial_train_indices, ]\n","\n","# Prepare a sequence of training sizes from the 70% training data\n","train_sizes <- seq(0.1, 1, by = 0.1) * nrow(train_data_initial)\n","accuracy_lasso <- numeric(length(train_sizes))\n","accuracy_ridge <- numeric(length(train_sizes))\n","\n","# Loop over the sequence to generate models and calculate TPR\n","for (i in seq_along(train_sizes)) {\n","  # Sample indices for creating training sets of different sizes from the 70% subset\n","  train_indices <- sample(nrow(train_data_initial), train_sizes[i])\n","  train_data <- train_data_initial[train_indices, ]\n","\n","  # Ensure all 'market' levels present in validation are in training\n","  market_level <- unique(train_data$market)\n","  valid_data <- valid_data %>%\n","    mutate(market = case_when(\n","      market %in% market_level ~ market,\n","      TRUE ~ \"OTHER\"\n","    ))\n","  # Create the full model matrix from the adjusted data, then split\n","  full_data <- bind_rows(train_data, valid_data) # Bind rows to keep structure\n","  full_matrix <- model.matrix(~ . - 1 - perfect_rating_score, data = full_data)\n","\n","  # Number of train data rows may change due to filtering, recalculate it\n","  train_n <- nrow(train_data)\n","\n","  # Split the full matrix into training and validation matrices\n","  x_train <- full_matrix[1:train_n, ]\n","  x_valid <- full_matrix[(train_n + 1):nrow(full_matrix), ]\n","\n","  # Prepare the response variable\n","  y_full <- as.numeric(as.factor(full_data$perfect_rating_score)) - 1\n","\n","  # Split the response variable into training and validation\n","  y_train <- y_full[1:train_n]\n","  y_valid <- y_full[(train_n + 1):nrow(full_matrix)]\n","\n","  # Lasso model and accuracy\n","  lasso_cv_model <- cv.glmnet(x_train, y_train, family = \"binomial\", alpha = 1)\n","  best_lambda_lasso <- lasso_cv_model$lambda.min\n","  lasso_model <- glmnet(x_train, y_train, family = \"binomial\", alpha = 1, lambda = best_lambda_lasso)\n","  lasso_pred_probabilities <- predict(lasso_model, newx = x_valid, s = \"lambda.min\", type = \"response\")\n","  lasso_predictions <- ifelse(lasso_pred_probabilities > 0.5, 1, 0)\n","  lasso_accuracy_list[i]<- accuracy(lasso_predictions, y_valid)\n","  # ridge model and accuracy\n","  ridge_cv_model <- cv.glmnet(x_train, y_train, family = \"binomial\", alpha = 0)\n","  best_lambda_ridge <- ridge_cv_model$lambda.min\n","  ridge_model <- glmnet(x_train, y_train, family = \"binomial\", alpha = 0, lambda = best_lambda_ridge)\n","  ridge_pred_probabilities <- predict(ridge_model, newx = x_valid, s = \"lambda.min\", type = \"response\")\n","  ridge_predictions <- ifelse(ridge_pred_probabilities > 0.5, 1, 0)\n","  ridge_accuracy_list[i]<- accuracy(ridge_predictions, y_valid)\n","}\n","\n","# Create a dataframe for plotting\n","plot_data <- data.frame(\n","  TrainSize = rep(train_sizes, 2),\n","  Accuracy = c(lasso_accuracy_list, ridge_accuracy_list),\n","  Model = rep(c(\"Lasso\", \"Ridge\"), each = length(train_sizes))\n",")\n","\n","# Plot the learning curve with Accuracy for both models\n","ggplot(plot_data, aes(x = TrainSize, y = Accuracy, color = Model)) +\n","  geom_line() +\n","  geom_point() +\n","  labs(x = \"Training Size\", y = \"Accuracy\",\n","       title = \"Learning Curve of Lasso and Ridge Models (Accuracy)\") +\n","  theme_minimal()+\n","  theme(panel.grid  = (element_line(color = \"grey95\")))"],"metadata":{"id":"8F3o2bB7sBJN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Complexity vs. Error (Lasso vs. Ridge)"],"metadata":{"id":"YTi_JG9Zx8DS"}},{"cell_type":"code","source":["################################# Plotting the cross-validation results #################################\n","# Extract the lambda values and corresponding cross-validated errors\n","cv_model_full_lasso <- cv.glmnet(x_train_test, y_train_test, family = \"binomial\", alpha = 1)\n","lambda_values_lasso <- cv_model_full_lasso$lambda\n","cv_errors_lasso <- cv_model_full_lasso$cvm\n","\n","cv_model_full_ridge <- cv.glmnet(x_train_test, y_train_test, family = \"binomial\", alpha = 0)\n","lambda_values_ridge <- cv_model_full_ridge$lambda\n","cv_errors_ridge <- cv_model_full_ridge$cvm\n","\n","# Prepare the data frames\n","plot_data_lasso <- data.frame(Model = \"Lasso\", Lambda = lambda_values_lasso, Error = cv_errors_lasso)\n","plot_data_ridge <- data.frame(Model = \"Ridge\", Lambda = lambda_values_ridge, Error = cv_errors_ridge)\n","\n","# Combine the data frames\n","plot_data <- rbind(plot_data_lasso, plot_data_ridge)\n","\n","# Plotting model complexity vs. error\n","ggplot(plot_data, aes(x = log(Lambda), y = Error, color = Model)) +\n","  geom_line() +\n","  geom_point(size = 2) +\n","  scale_color_manual(values = c(\"Lasso\" = \"#FF6666\", \"Ridge\" = \"#66B3FF\")) +\n","  labs(x = \"Log(Lambda) - Model Complexity\", y = \"Cross-Validated Error\",\n","       title = \"Error vs. Model Complexity for Lasso and Ridge Models\") +\n","  theme_minimal()"],"metadata":{"id":"MRkbAmc7xyLH"},"execution_count":null,"outputs":[]}]}